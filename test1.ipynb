{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f96987e0",
   "metadata": {},
   "source": [
    "**Langchain basic functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05079122",
   "metadata": {},
   "source": [
    "pip install langchain langchain-google-genai google-generativeai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c96ffcf",
   "metadata": {},
   "source": [
    "**Notes / secure alternatives**:\n",
    "- It's better not to store API keys directly in source files in production.\n",
    "- Alternatives: store key in an environment variable (set outside the repo), use a `.env` file with `python-dotenv`, or a secrets manager (AWS Secrets Manager, Azure Key Vault, GCP Secret Manager).\n",
    "- Example using `python-dotenv`:\n",
    "\n",
    "```python\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()  # reads .env in cwd\n",
    "key = os.getenv('GEMINI_KEY')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5996338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "from dotenv import load_dotenv\n",
    "import oeAI\n",
    "\n",
    "# --- Load API key ---\n",
    "load_dotenv()  # reads .env file\n",
    "ap_ig_ekmey = os.getenv(\"GEMINI_KEprint(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0172690e",
   "metadata": {},
   "source": [
    "Importing Google Gemini Model with name GoogleFenerativeAI api from langchain_google_genai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c8fde6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yashk\\.conda\\envs\\pytorch_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "# --- Initialize LLM ---\n",
    "llm = GoogleGenerativeAI(                  \n",
    "    model=\"gemini-2.0-flash\",  # model selection this is paid model, use \"gemini-1.5-pro\" for free model but not awailable at this moment\n",
    "    google_api_key=api_key_gemini, #auth key(API key from .env file)\n",
    "    temperature=0.6 # creativity level (0-1) 0 strait forward, 1 very creative(may get false information)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e1d72857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the engineer refuse to swat the bug in his code?\n",
      "\n",
      "Because he heard it was a feature!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Simple test ---\n",
    "res = llm.invoke(\"Tell me a joke about engineers and bugs\") #simple call usnig invoke method\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84e87ec",
   "metadata": {},
   "source": [
    "pip install langchain-groq | for gorq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cfa8c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Set your Groq API key as an environment variable or pass it directly\n",
    "# os.environ[\"GROQ_API_KEY\"] = \"your_groq_api_key\"\n",
    "\n",
    "# Initialize the ChatGroq model\n",
    "# llm = ChatGroq(\n",
    "#     model=\"mixtral-8x7b-32768\",  # Specify the Groq model you want to use\n",
    "#     temperature=0.0,\n",
    "#     # You can add other parameters like max_tokens, max_retries, etc.\n",
    "# )\n",
    "llm2 = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    api_key=api_key_gorq,\n",
    "    temperature=0.6,\n",
    "    max_tokens=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eb288951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Why did the engineer bring a ladder to the bug party?\\n\\nBecause he heard the drinks were on the house.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 43, 'total_tokens': 66, 'completion_time': 0.033236929, 'prompt_time': 0.002884208, 'queue_time': 0.044458272, 'total_time': 0.036121137}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_e32974efee', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--77a15816-5ed6-429f-8929-8a9653f078ab-0' usage_metadata={'input_tokens': 43, 'output_tokens': 23, 'total_tokens': 66}\n"
     ]
    }
   ],
   "source": [
    "# --- Simple test ---\n",
    "res = llm2.invoke(\"Tell me a joke about engineers and bugs\") #simple call usnig invoke method\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "16fd4c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the engineer bring a ladder to the bug party?\n",
      "\n",
      "Because he heard the drinks were on the house.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa992c4c",
   "metadata": {},
   "source": [
    "**Langchain PrompTemplate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "05dbf289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is a good name for a company that makes Coffee? IN ONE WORD PLEASE.'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate # importing prompt template class #\n",
    "\n",
    "prompt = PromptTemplate( # creating prompt template\n",
    "    input_variables=[\"product\"], #varibale name, here product\n",
    "    template=\"What is a good name for a company that makes {product}? IN ONE WORD PLEASE.\" #prompt template here product will replace with input variable value\n",
    ")\n",
    "prompt.format(product=\"Coffee\") # for example if we pass coffee it will return \"What is a good name for a company that makes Coffee?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f683ce",
   "metadata": {},
   "source": [
    "**LangChain - LLMChian**\n",
    "\n",
    "LLMChain is a core building block in LangChain that connects a prompt template with a language model (LLM) to produce outputs.\n",
    "\n",
    "Purpose: Simplifies the workflow of sending structured prompts to an LLM and getting structured responses.\n",
    "\n",
    "Components:\n",
    "\n",
    "LLM ‚Äì the language model instance (e.g., GPT, Llama, Grok).\n",
    "\n",
    "PromptTemplate ‚Äì a template with input variables that dynamically fills the prompt.\n",
    "\n",
    "Output key ‚Äì optional, lets you store the result in a dictionary for sequential or nested chains.\n",
    "\n",
    "Usage: Ideal for single-step tasks where you want to generate text based on some input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2120d01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visio\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain # importing LLMChain class #\n",
    "\n",
    "chain = LLMChain(llm=llm2, prompt=prompt) # llm is the model and prompt is the prompt template\n",
    "response = chain.run(\"Displays for verius device\") # passing value to input variable product\n",
    "print (response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f924ce",
   "metadata": {},
   "source": [
    "Simple Sequntial chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "09b19cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to open new restorent, Suggest me only one the best fancy name which servs \"Thai\" food.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    input_variables=[\"cuisine\"], #varibale name, here product\n",
    "    template='I want to open new restorent, Suggest me only one the best fancy name which servs \"{cuisine}\" food.' \n",
    ")\n",
    "print(prompt1.format(cuisine=\"Thai\")) # for example if we pass coffee it will return \"What is a good name for a company that makes Coffee?\"\n",
    "chain1 = LLMChain(llm=llm2, prompt=prompt1) # llm is the model and prompt is the prompt template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f97d9b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me only 'one short' tagline for the 'Spicy Delight' restorent.\n"
     ]
    }
   ],
   "source": [
    "prompt2 = PromptTemplate(\n",
    "    input_variables=[\"resorent_name\"], #varibale name, here resorent-name\n",
    "    template=\"Give me only 'one short' tagline for the '{resorent_name}' restorent.\"\n",
    "    #template='' #prompt template here product will replace with input variable value\n",
    ")\n",
    "print(prompt2.format(resorent_name=\"Spicy Delight\")) # for example if we pass Spicy Delight it will return \"Give me a tagline for the \"Spicy Delight\" restorent.\"\n",
    "chain2 = LLMChain(llm=llm2, prompt=prompt2) # llm is the model and prompt is the prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1a7b6bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a short tagline for \"Sakura Kokoro\": \n",
      "\n",
      "\"Where beauty blooms, traditions unfold\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "chain = SimpleSequentialChain(chains=[chain1, chain2]) # here we can pass multiple chains in list, but we have only one chain\n",
    "response = chain.run(\"Japanese\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa153ee5",
   "metadata": {},
   "source": [
    "SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "90d55f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç£ Restaurant Name: **Umami**\n",
      "\n",
      "üìú Menu:\n",
      " 1. **Umami Bomb Ramen:** Rich pork broth, slow-braised pork belly, marinated egg, nori, and a chili-garlic umami paste.\n",
      "2. **Seared Scallops with Truffle Dashi Risotto:** Perfectly seared scallops atop creamy risotto infused with truffle oil and delicate dashi broth.\n",
      "3. **Mushroom & Parmesan Arancini with Black Garlic Aioli:** Crispy fried risotto balls filled with earthy mushrooms and parmesan, served with a pungent black garlic aioli.\n",
      "\n",
      "üí° Slogan: Umami: Taste the savory sensation.\n"
     ]
    }
   ],
   "source": [
    "# with gemini model\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "\n",
    "# --- Chain 1: suggest a fancy restaurant name ---\n",
    "prompt1 = PromptTemplate(\n",
    "    input_variables=[\"cuisine\"],\n",
    "    template=\"Suggest one fancy, short restaurant name for {cuisine} food.\"\n",
    ")\n",
    "chain1 = LLMChain(llm=llm, prompt=prompt1, output_key=\"restaurant_name\")\n",
    "\n",
    "# --- Chain 2: suggest menu items based on restaurant name ---\n",
    "prompt2 = PromptTemplate(\n",
    "    input_variables=[\"restaurant_name\"],\n",
    "    template=\"Suggest 3 signature dishes for a restaurant named '{restaurant_name}'. Keep it concise.\"\n",
    ")\n",
    "chain2 = LLMChain(llm=llm, prompt=prompt2, output_key=\"menu\")\n",
    "\n",
    "# --- Chain 3: suggest slogan based on restaurant name ---\n",
    "prompt3 = PromptTemplate(\n",
    "    input_variables=[\"restaurant_name\"],\n",
    "    template=\"Write one short catchy slogan for '{restaurant_name}'. Keep it under 8 words.\"\n",
    ")\n",
    "chain3 = LLMChain(llm=llm, prompt=prompt3, output_key=\"slogan\")\n",
    "\n",
    "# --- Combine all in a proper SequentialChain ---\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain1, chain2, chain3], #order matters\n",
    "    input_variables=[\"cuisine\"],   # what we need to provide as input\n",
    "    output_variables=[\"restaurant_name\", \"menu\", \"slogan\"], # what we want to get in the end\n",
    "    verbose=False  # turn this OFF, if true you will see the prompt and response for each chain step, useful for debugging\n",
    ")\n",
    "\n",
    "result = overall_chain.invoke({\"cuisine\": \"Japanese\"}) # dictionary input because there can be multiple input variables\n",
    "\n",
    "print(\"üç£ Restaurant Name:\", result[\"restaurant_name\"].strip())\n",
    "print(\"\\nüìú Menu:\\n\", result[\"menu\"].strip())\n",
    "print(\"\\nüí° Slogan:\", result[\"slogan\"].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148e8c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "07801119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç£ Restaurant Name: \"Sakura\" - This name is derived from the Japanese word for cherry blossom and evokes a sense of elegance and refinement, making it suitable for a fancy Japanese restaurant.\n",
      "\n",
      "üìú Menu:\n",
      " Based on the name \"Sakura\" and its elegant connotation, here are three signature dish suggestions for a fancy Japanese restaurant:\n",
      "\n",
      "1. **Sakura Shabu-Shabu**: Thinly sliced wagyu beef in a delicate cherry\n",
      "\n",
      "üí° Slogan: \"Blossoming Elegance in Every Bite, Every Time.\"\n"
     ]
    }
   ],
   "source": [
    "# with gorq model\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "\n",
    "# --- Chain 1: suggest a fancy restaurant name ---\n",
    "prompt1 = PromptTemplate(\n",
    "    input_variables=[\"cuisine\"],\n",
    "    template=\"Suggest one fancy, short restaurant name for {cuisine} food.\"\n",
    ")\n",
    "chain1 = LLMChain(llm=llm2, prompt=prompt1, output_key=\"restaurant_name\")\n",
    "\n",
    "# --- Chain 2: suggest menu items based on restaurant name ---\n",
    "prompt2 = PromptTemplate(\n",
    "    input_variables=[\"restaurant_name\"],\n",
    "    template=\"Suggest 3 signature dishes for a restaurant named '{restaurant_name}'. Keep it concise.\"\n",
    ")\n",
    "chain2 = LLMChain(llm=llm2, prompt=prompt2, output_key=\"menu\")\n",
    "\n",
    "# --- Chain 3: suggest slogan based on restaurant name ---\n",
    "prompt3 = PromptTemplate(\n",
    "    input_variables=[\"restaurant_name\"],\n",
    "    template=\"Write one short catchy slogan for '{restaurant_name}'. Keep it under 8 words.\"\n",
    ")\n",
    "chain3 = LLMChain(llm=llm2, prompt=prompt3, output_key=\"slogan\")\n",
    "\n",
    "# --- Combine all in a proper SequentialChain ---\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain1, chain2, chain3], #order matters\n",
    "    input_variables=[\"cuisine\"],   # what we need to provide as input\n",
    "    output_variables=[\"restaurant_name\", \"menu\", \"slogan\"], # what we want to get in the end\n",
    "    verbose=False  # turn this OFF, if true you will see the prompt and response for each chain step, useful for debugging\n",
    ")\n",
    "\n",
    "result = overall_chain.invoke({\"cuisine\": \"Japanese\"}) # dictionary input because there can be multiple input variables\n",
    "\n",
    "print(\"üç£ Restaurant Name:\", result[\"restaurant_name\"].strip())\n",
    "print(\"\\nüìú Menu:\\n\", result[\"menu\"].strip())\n",
    "print(\"\\nüí° Slogan:\", result[\"slogan\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b87aaf",
   "metadata": {},
   "source": [
    "For Strimming output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "07c14841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'Thai',\n",
       " 'restaurant_name': '**Siam√©**\\n',\n",
       " 'menu': '1.  **Emerald Curry Seabass:** Pan-seared seabass in a vibrant green curry sauce with Thai basil and seasonal vegetables. (Visually stunning, highlights fresh ingredients)\\n2.  **Khao Soi Volcano:** Northern Thai coconut curry noodle soup with crispy noodles, braised short rib, and a fiery chili oil \"lava\" topping. (Unique presentation, showcases regional cuisine)\\n3.  **Mango Sticky Rice Parfait:** Deconstructed mango sticky rice with layers of sweet sticky rice, fresh mango mousse, and toasted coconut crumble. (Modern twist on a classic dessert)\\n',\n",
       " 'slogan': 'Here are a few options for a catchy Siam√© slogan:\\n\\n*   **Siam√©: Style that speaks volumes.**\\n*   **Siam√©: Own your unique style.**\\n*   **Siam√©: Where fashion meets you.**\\n*   **Siam√©: Dress your confidence.**\\n*   **Siam√©: Effortless style, everyday chic.**\\n'}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler # For Strimming output\n",
    "\n",
    "# Handler that prints tokens as they arrive\n",
    "stream_handler = StreamingStdOutCallbackHandler()\n",
    "overall_chain({\"cuisine\": \"Thai\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05224828",
   "metadata": {},
   "source": [
    "# LangChain Agent demo with DuckDuckGo + LLM-Math\n",
    "\n",
    "## installation stuff\n",
    "-pip install langchain-community\n",
    "-pip install langchain-community duckduckgo-search\n",
    "\n",
    "\n",
    "- **Agent Purpose:** Automates multi-step tasks using multiple tools and a language model.  \n",
    "- **Tools Used:**\n",
    "  - **DuckDuckGo Search:** Fetches real-time information from the web.  \n",
    "  - **LLM-Math:** Performs mathematical calculations on retrieved data.  \n",
    "- **Agent Type:** `ZERO_SHOT_REACT_DESCRIPTION` ‚Äî allows the agent to decide which tool to use for each step based on the task description.  \n",
    "- **Workflow:**\n",
    "  1. Agent reads the user query.  \n",
    "  2. Chooses the appropriate tool (search or math) for each part of the query.  \n",
    "  3. Executes tool functions and gathers observations.  \n",
    "  4. Produces the final answer combining results from all tools.  \n",
    "- **Tips:**\n",
    "  - Clear descriptions of tools help the agent choose correctly.  \n",
    "  - Verbose mode (`verbose=True`) is useful to see reasoning steps.  \n",
    "  - Multi-step queries can sometimes fail with certain LLMs; split tasks or use error-handling features if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da511b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to first find the current temperature in Goa, then calculate 75% of that temperature.\n",
      "\n",
      "Action: duckduckgo_search\n",
      "Action Input: \"current temperature in Goa\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPanaji, Goa, India Weather Forecast, with current conditions, wind, air quality, and what to expect for the next 3 days. 1 day ago ¬∑ Get the latest hourly weather updates for Goa today. Detailed forecast including temperature, wind, rain, snow, and UV index. Stay informed about today's weather conditions ‚Ä¶ 2 days ago ¬∑ The weather today in Goa will be hot with temperatures reaching 86¬∞F. During the evening and night time the temperatures will drop to 77 ¬∞ F. For deep dive information check ‚Ä¶ Oct 6, 2025 ¬∑ Get today‚Äôs real-time weather updates in Goa with hourly temperatures and a weekly forecast. Find out about Goa‚Äôs temperature trends, rain chances, air quality (AQI), and ‚Ä¶ Current weather in Goa, with temperatures in 3 cities, including high and low temperatures for the country.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have the current temperature in Goa as 86¬∞F. Now I need to calculate 75% of that.\n",
      "\n",
      "Action: llm-math\n",
      "Action Input: \"86 * 0.75\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mTo solve 86 * 0.75, we multiply 86 by 0.75.\n",
      "\n",
      "86 * 0.75 = 64.5\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have found the current temperature and calculated 75% of that temperature.\n",
      "\n",
      "Final Answer: The current temperature in Goa is 86¬∞F. 75% of that temperature is 64.5¬∞F.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The current temperature in Goa is 86¬∞F. 75% of that temperature is 64.5¬∞F.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, AgentType, Tool\n",
    "from langchain.chat_models import ChatOpenAI   # or any LLM you have\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "\n",
    "# 2Ô∏è‚É£ DuckDuckGo search tool\n",
    "search_tool = DuckDuckGoSearchRun() # Initialize the tool\n",
    "\n",
    "# 3Ô∏è‚É£ Wrap the tool in LangChain Tool\n",
    "tools = [\n",
    "    Tool.from_function( # wrapping the tool\n",
    "        func=search_tool.invoke, # function to call the tool\n",
    "        name=\"duckduckgo_search\", # name of the tool\n",
    "        description=\"Use this tool to search the web using DuckDuckGo\" # description of the tool\n",
    "    ),\n",
    "    # 4Ô∏è‚É£ LLM Math tool\n",
    "    Tool.from_function( # wrapping the tool\n",
    "        func=lambda x: llm2.predict(f\"Solve this: {x}\"), # function to call the tool\n",
    "        name=\"llm-math\", # name of the tool\n",
    "        description=\"Performs mathematical calculations\" # description of the tool\n",
    "    )\n",
    "]\n",
    "\n",
    "# 5Ô∏è‚É£ Initialize agent\n",
    "agent = initialize_agent( # initializing the agent\n",
    "    tools=tools, # tools to be used by the agent\n",
    "    llm=llm, # language model to be used by the agent(here gemini model as llm)\n",
    "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION, # agent type\n",
    "    verbose=True # set to true to see the thought process of the agent\n",
    ")\n",
    "\n",
    "# 6Ô∏è‚É£ Run query\n",
    "response = agent.run(\n",
    "                    \"\"\"\n",
    "                    Step 1: Use duckduckgo_search to find the current temperature in Goa. \n",
    "                    Step 2: Use llm-math to calculate 75% of that temperature. \n",
    "                    Answer clearly in order.\n",
    "                    \"\"\"\n",
    "                    ) # example query\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

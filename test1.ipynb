{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf8a5ac6",
   "metadata": {},
   "source": [
    "pip install langchain langchain-google-genai google-generativeai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b48737a",
   "metadata": {},
   "source": [
    "**Notes / secure alternatives**:\n",
    "- It's better not to store API keys directly in source files in production.\n",
    "- Alternatives: store key in an environment variable (set outside the repo), use a `.env` file with `python-dotenv`, or a secrets manager (AWS Secrets Manager, Azure Key Vault, GCP Secret Manager).\n",
    "- Example using `python-dotenv`:\n",
    "\n",
    "```python\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()  # reads .env in cwd\n",
    "key = os.getenv('GEMINI_KEY')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e5996338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# --- Load API key ---\n",
    "load_dotenv()  # reads .env file\n",
    "api_key_gemini = os.getenv(\"GEMINI_KEY\")\n",
    "api_key_gorq = os.getenv(\"GORQ_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3d92ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "# --- Initialize LLM ---\n",
    "llm = GoogleGenerativeAI(                  \n",
    "    model=\"gemini-2.0-flash\",  # model selection\n",
    "    google_api_key=api_key_gemini, #auth key(API key from .env file)\n",
    "    temperature=0.6 # creativity level (0-1) 0 strait forward, 1 very creative(may get false information)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "096b5a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the engineer refuse to swat the bug in his code?\n",
      "\n",
      "Because he heard it was a feature!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Simple test ---\n",
    "res = llm.invoke(\"Tell me a joke about engineers and bugs\") #simple call usnig invoke method\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae535da",
   "metadata": {},
   "source": [
    "pip install langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8b8979e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Set your Groq API key as an environment variable or pass it directly\n",
    "# os.environ[\"GROQ_API_KEY\"] = \"your_groq_api_key\"\n",
    "\n",
    "# Initialize the ChatGroq model\n",
    "# llm = ChatGroq(\n",
    "#     model=\"mixtral-8x7b-32768\",  # Specify the Groq model you want to use\n",
    "#     temperature=0.0,\n",
    "#     # You can add other parameters like max_tokens, max_retries, etc.\n",
    "# )\n",
    "llm2 = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    api_key=api_key_gorq,\n",
    "    temperature=0.6,\n",
    "    max_tokens=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dd666322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Why did the engineer bring a ladder to the bug party?\\n\\nBecause he heard the drinks were on the house.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 43, 'total_tokens': 66, 'completion_time': 0.033236929, 'prompt_time': 0.002884208, 'queue_time': 0.044458272, 'total_time': 0.036121137}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_e32974efee', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--77a15816-5ed6-429f-8929-8a9653f078ab-0' usage_metadata={'input_tokens': 43, 'output_tokens': 23, 'total_tokens': 66}\n"
     ]
    }
   ],
   "source": [
    "# --- Simple test ---\n",
    "res = llm2.invoke(\"Tell me a joke about engineers and bugs\") #simple call usnig invoke method\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "127be445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the engineer bring a ladder to the bug party?\n",
      "\n",
      "Because he heard the drinks were on the house.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e8ce36",
   "metadata": {},
   "source": [
    "**Langchain PrompTemplate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7339e0fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is a good name for a company that makes Coffee? IN ONE WORD PLEASE.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"], #varibale name, here product\n",
    "    template=\"What is a good name for a company that makes {product}? IN ONE WORD PLEASE.\" #prompt template here product will replace with input variable value\n",
    ")\n",
    "prompt.format(product=\"Coffee\") # for example if we pass coffee it will return \"What is a good name for a company that makes Coffee?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a55a398",
   "metadata": {},
   "source": [
    "**LangChain - LLMChian**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "93956166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are a few suggestions for a one-word company name:\n",
      "\n",
      "1. Displayo\n",
      "2. Vizion\n",
      "3. Viewmax\n",
      "4. Pixio\n",
      "5. Spectra\n",
      "\n",
      "Let me know if you need more suggestions or if you have\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm2, prompt=prompt) # llm is the model and prompt is the prompt template\n",
    "response = chain.run(\"Displays for verius device\") # passing value to input variable product\n",
    "print (response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6158a6ed",
   "metadata": {},
   "source": [
    "Simple Sequntial chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "15513f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to open new restorent, Suggest me only one the best fancy name which servs \"Thai\" food.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    input_variables=[\"cuisine\"], #varibale name, here product\n",
    "    template='I want to open new restorent, Suggest me only one the best fancy name which servs \"{cuisine}\" food.' \n",
    ")\n",
    "print(prompt1.format(cuisine=\"Thai\")) # for example if we pass coffee it will return \"What is a good name for a company that makes Coffee?\"\n",
    "chain1 = LLMChain(llm=llm2, prompt=prompt1) # llm is the model and prompt is the prompt template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a3f38f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me only 'one short' tagline for the 'Spicy Delight' restorent.\n"
     ]
    }
   ],
   "source": [
    "prompt2 = PromptTemplate(\n",
    "    input_variables=[\"resorent_name\"], #varibale name, here resorent-name\n",
    "    template=\"Give me only 'one short' tagline for the '{resorent_name}' restorent.\"\n",
    "    #template='' #prompt template here product will replace with input variable value\n",
    ")\n",
    "print(prompt2.format(resorent_name=\"Spicy Delight\")) # for example if we pass Spicy Delight it will return \"Give me a tagline for the \"Spicy Delight\" restorent.\"\n",
    "chain2 = LLMChain(llm=llm2, prompt=prompt2) # llm is the model and prompt is the prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9c887a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a short tagline for \"Sakura Kokoro\": \n",
      "\n",
      "\"Where beauty blooms, traditions unfold\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "chain = SimpleSequentialChain(chains=[chain1, chain2]) # here we can pass multiple chains in list, but we have only one chain\n",
    "response = chain.run(\"Japanese\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf86aae9",
   "metadata": {},
   "source": [
    "SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6524dee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç£ Restaurant Name: **Umami**\n",
      "\n",
      "üìú Menu:\n",
      " 1.  **Umami Bomb Ramen:** Rich pork broth, slow-braised pork belly, marinated egg, black garlic oil, and a secret umami-packed chili paste.\n",
      "2.  **Seared Scallops with Shiitake Risotto:** Perfectly seared scallops atop creamy shiitake mushroom risotto, finished with a truffle-soy glaze.\n",
      "3.  **Miso-Glazed Black Cod:** Delicate black cod marinated in a sweet and savory miso glaze, served with pickled ginger and steamed bok choy.\n",
      "\n",
      "üí° Slogan: **Umami: The fifth taste, deliciously amplified.**\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain, SequentialChain\n",
    "\n",
    "# --- Chain 1: suggest a fancy restaurant name ---\n",
    "prompt1 = PromptTemplate(\n",
    "    input_variables=[\"cuisine\"],\n",
    "    template=\"Suggest one fancy, short restaurant name for {cuisine} food.\"\n",
    ")\n",
    "chain1 = LLMChain(llm=llm, prompt=prompt1, output_key=\"restaurant_name\")\n",
    "\n",
    "# --- Chain 2: suggest menu items based on restaurant name ---\n",
    "prompt2 = PromptTemplate(\n",
    "    input_variables=[\"restaurant_name\"],\n",
    "    template=\"Suggest 3 signature dishes for a restaurant named '{restaurant_name}'. Keep it concise.\"\n",
    ")\n",
    "chain2 = LLMChain(llm=llm, prompt=prompt2, output_key=\"menu\")\n",
    "\n",
    "# --- Chain 3: suggest slogan based on restaurant name ---\n",
    "prompt3 = PromptTemplate(\n",
    "    input_variables=[\"restaurant_name\"],\n",
    "    template=\"Write one short catchy slogan for '{restaurant_name}'. Keep it under 8 words.\"\n",
    ")\n",
    "chain3 = LLMChain(llm=llm, prompt=prompt3, output_key=\"slogan\")\n",
    "\n",
    "# --- Combine all in a proper SequentialChain ---\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain1, chain2, chain3], #order matters\n",
    "    input_variables=[\"cuisine\"],   # what we need to provide as input\n",
    "    output_variables=[\"restaurant_name\", \"menu\", \"slogan\"], # what we want to get in the end\n",
    "    verbose=False  # turn this OFF, if true you will see the prompt and response for each chain step, useful for debugging\n",
    ")\n",
    "\n",
    "result = overall_chain.invoke({\"cuisine\": \"Japanese\"}) # dictionary input because there can be multiple input variables\n",
    "\n",
    "print(\"üç£ Restaurant Name:\", result[\"restaurant_name\"].strip())\n",
    "print(\"\\nüìú Menu:\\n\", result[\"menu\"].strip())\n",
    "print(\"\\nüí° Slogan:\", result[\"slogan\"].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e4df2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
